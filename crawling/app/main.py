#!/usr/bin/env python
# coding: utf-8

# ## Default Setting

# In[15]:


import os
from pathlib import Path
import requests
from bs4 import BeautifulSoup
import re
from konlpy.tag import Okt
import sqlite3
from fastapi import FastAPI
from pydantic import BaseModel
from typing import List, Optional
from collections import OrderedDict


# ğŸ”¹ ì œì™¸í•  ì˜ì–´ ê´€ì‚¬(articles) ëª©ë¡
ARTICLES = {"a", "an", "the"}

# ğŸ”¹ í•œê¸€ ì¡°ì‚¬ ëª©ë¡ (ë‹¨ì–´ ëì— ë¶™ëŠ” ê²½ìš° ì˜ë¯¸ ë¶„ì„ ë°©í•´ â†’ ì œê±° ëŒ€ìƒ)
PARTICLES = {
    "ì€",
    "ëŠ”",
    "ì´",
    "ê°€",
    "ì„",
    "ë¥¼",
    "ì—",
    "ì—ì„œ",
    "ì™€",
    "ê³¼",
    "ë„",
    "ì˜",
    "í•œ",
    "ë¡œ",
    "ìœ¼ë¡œ",
    "í•˜ê³ ",
    "ë°",
    "ë“±",
    "ê¹Œì§€",
    "ë¶€í„°",
    "ë§Œ",
    "ë³´ë‹¤",
    "ì²˜ëŸ¼",
    "ê°™ì´",
    "ê»˜ì„œ",
}


# ## URL Parsing

# In[16]:




# âœ… ì„ íƒí•œ ê²½ì œ ì–¸ë¡ ì‚¬ ëª©ë¡
ECONOMY_PRESS_LIST = ["í•œêµ­ê²½ì œ", "ë§¤ì¼ê²½ì œ", "ì„œìš¸ê²½ì œ", "ë¨¸ë‹ˆíˆ¬ë°ì´", "íŒŒì´ë‚¸ì…œë‰´ìŠ¤"]

def parse_time_to_minutes(time_str):
    """
    '1ë¶„ì „', '2ì‹œê°„ì „', 'ì–´ì œ' ë“± ë¬¸ìì—´ì„ ë¶„ ë‹¨ìœ„ë¡œ ë³€í™˜
    5ë¶„ ê¸°ì¤€ í•„í„°ë§ì—ë§Œ ì‚¬ìš©
    """
    if "ë¶„ì „" in time_str:
        return int(re.search(r'(\d+)', time_str).group(1))          
    elif "ì‹œê°„ì „" in time_str:
        return int(re.search(r'(\d+)', time_str).group(1)) * 60
    else:
        return 9999  # 'ì–´ì œ', '3ì¼ì „' ë“±ì€ ì•„ì£¼ í° ê°’ìœ¼ë¡œ ì²˜ë¦¬

def get_naver_economy_news_urls_from_list(pages=10, allowed_press=ECONOMY_PRESS_LIST, max_minutes=100):
    base_url = "https://news.naver.com/main/list.naver"
    headers = {"User-Agent": "Mozilla/5.0"}

    all_results = []
    seen = set()

    for page in range(1, pages + 1):
        params = {"mode": "LSD", "mid": "shm", "sid1": "101", "page": str(page)}
        response = requests.get(base_url, headers=headers, params=params)
        if response.status_code != 200:
            continue

        soup = BeautifulSoup(response.text, "html.parser")
        news_blocks = soup.select("ul.type06_headline li") + soup.select("ul.type06 li")

        for block in news_blocks:
            a_tag = block.select_one("dt > a")
            press_tag = block.select_one("span.writing")
            time_tag = block.select_one("dd > span.date")
            if not a_tag or not press_tag or not time_tag:
                continue

            href = a_tag.get("href")
            press_name = press_tag.get_text(strip=True)
            time_text = time_tag.get_text(strip=True)

            minutes = parse_time_to_minutes(time_text)
            if minutes <= max_minutes and any(name in press_name for name in allowed_press):
                if href not in seen:
                    seen.add(href)
                    # âœ… URLë§Œ ì €ì¥
                    all_results.append(href)

    print(f"âœ… ì´ {len(all_results)}ê°œì˜ ìµœê·¼ ë‰´ìŠ¤ ë§í¬ ìˆ˜ì§‘ ì™„ë£Œ")
    return all_results



# ## Word crawling

# In[17]:


# ë‰´ìŠ¤ ë³¸ë¬¸ì„ í¬ë¡¤ë§í•˜ëŠ” í•¨ìˆ˜
def get_news_text(url: str) -> str:
    headers = {
        # í¬ë¡¤ë§ ì‹œ ì°¨ë‹¨ì„ í”¼í•˜ê¸° ìœ„í•œ User-Agent ì„¤ì •
        "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) "
                      "AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36"
    }
    response = requests.get(url, headers=headers)  # URL ìš”ì²­
    if response.status_code != 200:
        print(f"âŒ í¬ë¡¤ë§ ì‹¤íŒ¨: {url} ìƒíƒœì½”ë“œ {response.status_code}")
        return ""  # ì‹¤íŒ¨ ì‹œ ë¹ˆ ë¬¸ìì—´ ë°˜í™˜
    
    soup = BeautifulSoup(response.text, "html.parser")  # HTML íŒŒì‹±
    
    # ë„¤ì´ë²„ ë‰´ìŠ¤ ë³¸ë¬¸ì€ ë³´í†µ 'div#newsct_article' ì•ˆì— ìˆìŒ
    article_body = soup.select_one("div#newsct_article")
    if article_body:
        # ë³¸ë¬¸ í…ìŠ¤íŠ¸ë¥¼ ì¤„ë°”ê¿ˆ í¬í•¨í•´ ê¹”ë”í•˜ê²Œ ì¶”ì¶œ
        return article_body.get_text(strip=True, separator="\n")
    else:
        print(f"âŒ ë³¸ë¬¸ ì—†ìŒ: {url}")
        return ""  # ë³¸ë¬¸ ì—†ìœ¼ë©´ ë¹ˆ ë¬¸ìì—´ ë°˜í™˜
    
# ë‰´ìŠ¤ ì œëª©ê³¼ ëŒ€í‘œ ì´ë¯¸ì§€ URLì„ ì¶”ì¶œí•˜ëŠ” í•¨ìˆ˜
def get_title_and_image(url: str):
    headers = {
        "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) "
                      "AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36"
    }
    response = requests.get(url, headers=headers)
    soup = BeautifulSoup(response.text, "html.parser")

    # ì œëª©: id=title_area ì•ˆì˜ h2 > span
    title_tag = soup.select_one("h2#title_area span")
    title = title_tag.text.strip() if title_tag else "ì œëª© ì—†ìŒ"

    # ëŒ€í‘œ ì´ë¯¸ì§€: ì—¬ëŸ¬ í›„ë³´ ìœ„ì¹˜ í™•ì¸
    image_tag = (
        soup.select_one("figure img") or
        soup.select_one("span.end_photo_org img") or
        soup.select_one("div#newsct_article img")
    )

    if image_tag and "src" in image_tag.attrs:
        image_url = image_tag["src"]
    else:
        # fallback: <meta property="og:image"> í™•ì¸
        meta_tag = soup.find("meta", property="og:image")
        image_url = meta_tag["content"] if meta_tag else None

    return title, image_url


def extract_news_metadata(url: str):
    headers = {
        "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64)"
    }
    response = requests.get(url, headers=headers)
    if response.status_code != 200:
        print(f"âŒ ìš”ì²­ ì‹¤íŒ¨: {url} ìƒíƒœì½”ë“œ {response.status_code}")
        return None, None, None, None

    soup = BeautifulSoup(response.text, "html.parser")

      # ğŸ“° ì œëª© ì¶”ì¶œ
    title_tag = soup.select_one("h2#title_area span")
    title = title_tag.text.strip() if title_tag else "ì œëª© ì—†ìŒ"

    image_tag = (
        soup.select_one("figure img") or
        soup.select_one("span.end_photo_org img") or
        soup.select_one("div#newsct_article img")
    )

    if image_tag and "src" in image_tag.attrs:
        image_url = image_tag["src"]
    else:
        # fallback: <meta property="og:image"> í™•ì¸
        meta_tag = soup.find("meta", property="og:image")
        image_url = meta_tag["content"] if meta_tag else None
    # ì–¸ë¡ ì‚¬
    press_tag = soup.select_one("a.media_end_head_top_logo img")
    press = press_tag.get("alt").strip() if press_tag and press_tag.has_attr("alt") else "ì–¸ë¡ ì‚¬ ë¯¸í™•ì¸"

    # ë‚ ì§œ
    time_tag = soup.select_one("span.media_end_head_info_datestamp_time") or soup.select_one("span.t11")
    date = time_tag.get_text(strip=True) if time_tag else "ë‚ ì§œ ë¯¸í™•ì¸"

    return title, image_url, press, date

# ## Remove particles

# In[18]:

def extract_press_name(soup: BeautifulSoup) -> str:
    """
    ë„¤ì´ë²„ ë‰´ìŠ¤ ê¸°ì‚¬ í˜ì´ì§€ì—ì„œ ì–¸ë¡ ì‚¬ ì´ë¦„ ì¶”ì¶œ
    """
    press_tag = soup.select_one("a.media_end_head_top_logo img")
    return press_tag.get("alt").strip() if press_tag and press_tag.has_attr("alt") else "ì–¸ë¡ ì‚¬ ë¯¸í™•ì¸"


def extract_article_date(soup: BeautifulSoup) -> str:
    """
    ë„¤ì´ë²„ ë‰´ìŠ¤ ê¸°ì‚¬ í˜ì´ì§€ì—ì„œ ê¸°ì‚¬ ì‘ì„± ë‚ ì§œ ì¶”ì¶œ
    """
    time_tag = soup.select_one("span.media_end_head_info_datestamp_time") \
               or soup.select_one("span.t11")
    return time_tag.get_text(strip=True) if time_tag else "ë‚ ì§œ ë¯¸í™•ì¸"

def remove_particles(word):
    """
    í•œê¸€ ë‹¨ì–´ì—ì„œ ì¡°ì‚¬(ì¡°ì‚¬ ëª©ë¡ì— í¬í•¨ëœ ë‹¨ì–´)ë¥¼ ì œê±°í•˜ëŠ” í•¨ìˆ˜
    ì˜ˆ) "ì „ì‚°ì¥ì• ë¡œ" â†’ "ì „ì‚°ì¥ì• "
    """
    # ì •ê·œí‘œí˜„ì‹ì„ í†µí•´ ë‹¨ì–´ ëì— ë¶™ì€ ì¡°ì‚¬ ì œê±°
    pattern = r"(" + "|".join(PARTICLES) + r")$"
    return re.sub(pattern, "", word)



def extract_words_okt (text):
    """
    ë³¸ë¬¸ì—ì„œ ëª…ì‚¬ë¥¼ ì¶”ì¶œí•˜ê³  ì¡°ì‚¬ ì œê±° í›„
    ì¤‘ë³µ ì—†ì´ ì²˜ë¦¬í•˜ì§€ ì•Šê³  ë“±ì¥ ìˆœì„œ ê·¸ëŒ€ë¡œ ë°˜í™˜
    """
    okt = Okt()
    nouns = okt.phrases(text)
    filtered = [remove_particles(n) for n in nouns if n.strip()]
    return filtered  # ì¤‘ë³µ ì œê±° ì—†ì´ ê·¸ëŒ€ë¡œ ë°˜í™˜



# ## Print Word List

# In[19]:


def print_words_in_rows(words, words_per_row=10):
    """
    ë‹¨ì–´ ë¦¬ìŠ¤íŠ¸ë¥¼ ì§€ì •ëœ ê°œìˆ˜(words_per_row)ë§Œí¼ í•œ ì¤„ì— ì¶œë ¥
    """
    for i in range(0, len(words), words_per_row):
        print(", ".join(words[i : i + words_per_row]))


# ## DB Scanning

# In[20]:


BASE_DIR = Path(__file__).resolve().parent.parent
TERMS_DB_PATH = os.environ.get("TERMS_DB_PATH")
db_path = TERMS_DB_PATH if TERMS_DB_PATH else str(BASE_DIR / "economics_terms.db")

def find_description_from_db(conn, term_input):
    cursor = conn.cursor()
    query = "SELECT * FROM terms WHERE term = ?"
    cursor.execute(query, (term_input,))
    result = cursor.fetchone()
    if result:
        desc1 = result[2] if result[2] else ""
        desc2 = result[3] if result[3] else ""
        desc3 = result[4] if result[4] else ""
        return desc1, desc2, desc3
    else:
        return None, None, None

def extract_and_explain(url, db_path):
    # get_news_text, extract_words_okt í•¨ìˆ˜ëŠ” ì™¸ë¶€ì— ì •ì˜ë˜ì–´ ìˆì–´ì•¼ í•©ë‹ˆë‹¤.
    text = get_news_text(url)
    words_Konpy = extract_words_okt(text)
    words_Konpy = list(dict.fromkeys(words_Konpy))  # ì¤‘ë³µ ì œê±° (ìˆœì„œ ìœ ì§€)

    db_file = Path(db_path)
    if not db_file.is_absolute():
        db_file = BASE_DIR / db_file

    if not db_file.exists():
        raise FileNotFoundError(f"ê²½ì œ ìš©ì–´ DB íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {db_file}")

    conn = sqlite3.connect(db_file)
    terms = []  # (ë‹¨ì–´, í•´ì„¤1, í•´ì„¤2, í•´ì„¤3) ì €ì¥ìš© ë¦¬ìŠ¤íŠ¸

    try:
        for word in words_Konpy:
            desc1, desc2, desc3 = find_description_from_db(conn, word)
            if any([desc1, desc2, desc3]):
                terms.append({"term": word, "desc1": desc1, "desc2": desc2, "desc3": desc3})
    finally:
        conn.close()

    # if terms:
    #     print("ğŸ” ê¸°ì‚¬ì—ì„œ ë“±ì¥í•œ ê²½ì œ ìš©ì–´ ì„¤ëª… Konlpy:\n")
    #     for t in terms:
    #         print(f"ğŸ“Œ {t['term']}:")
    #         print(f"    í•´ì„¤1: {t['desc1']}")
    #         print(f"    í•´ì„¤2: {t['desc2']}")
    #         print(f"    í•´ì„¤3: {t['desc3']}\n")
    # else:
    #     print("ğŸ“ ê¸°ì‚¬ ë‚´ì—ì„œ ì„¤ëª… ê°€ëŠ¥í•œ ê²½ì œ ìš©ì–´ë¥¼ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")

    return terms  # ì¶”í›„ ë‹¤ë¥¸ ê³³ì— í™œìš© ê°€ëŠ¥


# ## í¬ë¡¤ë§ & ë‹¨ì–´ matching test

# In[21]:


# âœ… ì‹¤í–‰
if __name__ == "__main__":

    # 1. ë‰´ìŠ¤ URL ìˆ˜ì§‘
    news_urls = get_naver_economy_news_urls_from_list(1)

    # 2. ê° ë‰´ìŠ¤ URLì— ëŒ€í•´ ì²˜ë¦¬
    for url in news_urls:
        extract_and_explain(url, db_path)


# ## Fast API Code

# In[ ]:


from fastapi import FastAPI, Query
from konlpy.tag import Okt
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity
import requests
from collections import OrderedDict

# ìˆ˜ì •
from pydantic import BaseModel
from typing import List, Dict

app = FastAPI()
#ìˆ˜ì •
NEWS_CACHE: Dict[int, Dict] = {}

# Spring ì„œë²„ ì£¼ì†Œ (ì»¨í…Œì´ë„ˆ í™˜ê²½ ê³ ë ¤)
SPRING_ENDPOINT = os.environ.get("SPRING_ENDPOINT", "http://backend:8080/news/upload")


okt = Okt()

STOPWORDS = set([
    "ê¸°ì", "ë°œí‘œ", "ê´€ë ¨", "ì´ë²ˆ", "ì´ë‚ ", "ê³„íš", "í†µí•´", "ë“±", "ë°", "ëŒ€í•œ"
])

def extract_nouns(text):
    nouns = okt.nouns(text)
    return " ".join([n for n in nouns if len(n) > 1 and n not in STOPWORDS])

def get_top_keywords(tfidf_matrix, feature_names, top_n=5, min_score=0.05):
    result = []
    for row in tfidf_matrix:
        row_array = row.toarray().flatten()
        top_indices = row_array.argsort()[::-1]
        keywords = []
        for idx in top_indices:
            if row_array[idx] < min_score:
                continue
            keywords.append(feature_names[idx])
            if len(keywords) >= top_n:
                break
        result.append(keywords)
    return result


# def recommend_similar_news(idx, tfidf_matrix, news_data, top_n=2):
#     cosine_sim = cosine_similarity(tfidf_matrix[idx], tfidf_matrix).flatten()
#     similar_indices = cosine_sim.argsort()[::-1][1:top_n+1]
#     return [news_data[i] for i in similar_indices]

#ìˆ˜ì •
class NewsCacheItem(BaseModel):
    news_id: int
    title: str
    url: str
    keywords: List[str]

class RecommendRequest(BaseModel):
    user_id: int
    clicked_news_ids: List[int]

class Recommendation(BaseModel):
    news_id: int
    title: str
    url: str
    matched_keywords: List[str]

class RecommendResponse(BaseModel):
    user_id: int
    recommendations: List[Recommendation]

@app.post("/crawl_auto/")
def crawl_auto(pages: int = Query(1, description="ê°€ì ¸ì˜¬ ë‰´ìŠ¤ í˜ì´ì§€ ìˆ˜")):
    print(f"ğŸ“¥ [1] {pages} í˜ì´ì§€ì— ëŒ€í•´ ë‰´ìŠ¤ í¬ë¡¤ë§ ì‹œì‘")

    # 1. ìë™ìœ¼ë¡œ URL ë¦¬ìŠ¤íŠ¸ ìˆ˜ì§‘
    urls = get_naver_economy_news_urls_from_list()
    news_data = []

    print(f"ğŸ”— [2] ì´ {len(urls)}ê°œ ë‰´ìŠ¤ URL ìˆ˜ì§‘ ì™„ë£Œ")

    for i, url in enumerate(urls):
        print(f"ğŸ“° [3.{i+1}] ë‰´ìŠ¤ URL í¬ë¡¤ë§ ì¤‘: {url}")
        title, image_url, press, date= extract_news_metadata(url)
        content = get_news_text(url)
        if content:
            news_data.append({
                "url": url,
                "title": title,
                "imageUrl": image_url,
                "content": content,
                "press": press,
                "date": date
            })
            print(f"âœ… [3.{i+1}] ì œëª©: {title}")
        else:
            print(f"âš ï¸ [3.{i+1}] ë³¸ë¬¸ì´ ë¹„ì–´ ìˆì–´ ì œì™¸")

    print(f"ğŸ§  [4] TF-IDF ë¶„ì„ ì‹œì‘: ì´ {len(news_data)}ê°œ ë‰´ìŠ¤")

    docs = [extract_nouns(n["content"]) for n in news_data]
    vectorizer = TfidfVectorizer()
    tfidf_matrix = vectorizer.fit_transform(docs)
    feature_names = vectorizer.get_feature_names_out()
    keywords_list = get_top_keywords(tfidf_matrix, feature_names)

    results = []

    print(f"ğŸ“¡ [5] FastAPI â†’ Spring ë°ì´í„° ì „ì†¡ ì‹œì‘")

    for i, item in enumerate(news_data):
        url = item["url"]
        title = item["title"]
        content = item["content"]
        image_url = item["imageUrl"]
        terms = extract_and_explain(url, db_path)
        keywords = keywords_list[i]

        print(f"\nğŸ“¦ [5.{i+1}] ì „ì†¡ ì¤€ë¹„ - ë‰´ìŠ¤: {title}")
        print(f"    ğŸ”‘ ì£¼ìš” í‚¤ì›Œë“œ: {keywords}")
        print(f"    ğŸ“˜ ìš©ì–´ ìˆ˜: {len(terms)}ê°œ")

        data = {
            "url": url,
            "title": title,
            "content": content,
            "imageUrl": image_url,
            "terms": terms,
            "keywords": keywords,
            "press": item["press"],
            "date": item["date"]
        }

        try:
            res = requests.post(SPRING_ENDPOINT, json=data, timeout=5)
            print(f" [5.{i+1}] ì „ì†¡ ì„±ê³µ â†’ ì‘ë‹µ ì½”ë“œ: {res.status_code}")
            results.append({
                "url": url,
                "status": res.status_code,
                "spring_response": res.text,
                "title": title,
                "imageUrl": image_url,
                "press": item["press"],
                "date": item["date"],
                "terms_found": [t["term"] for t in terms],
                "keywords": keywords
            })
        except Exception as e:
            print(f" [5.{i+1}] ì „ì†¡ ì‹¤íŒ¨: {e}")
            results.append({
                "url": url,
                "error": str(e),
                "message": "Spring ì„œë²„ë¡œ ì „ì†¡ ì‹¤íŒ¨"
            })

    print(f"\nğŸ‰ [6] ì „ì²´ ë‰´ìŠ¤ ì²˜ë¦¬ ì™„ë£Œ ({len(results)}ê°œ)")
    return {"results": results}

# ì‚¬ìš©ì ë§ì¶¤ ì¶”ì²œ í”¼ë“œ ì„ì‹œ êµ¬í˜„
#ë‰´ìŠ¤ ìºì‹± ì €ì¥ API
@app.post("/cache/update")
def update_cache(item: NewsCacheItem):
    print(f"\nğŸ“¥ ìºì‹œ ì—…ë°ì´íŠ¸ ìš”ì²­ ë°›ìŒ:")
    print(f"- ë‰´ìŠ¤ ID: {item.news_id}")
    print(f"- ì œëª©: {item.title}")
    print(f"- í‚¤ì›Œë“œ: {item.keywords}")
    
    NEWS_CACHE[item.news_id] = {
        "title": item.title,
        "url": item.url,
        "keywords": item.keywords
    }
    print(f"âœ… ë‰´ìŠ¤ {item.news_id} ìºì‹œì— ì €ì¥ ì™„ë£Œ")
    print(f"ğŸ“¦ í˜„ì¬ ìºì‹œ í¬ê¸°: {len(NEWS_CACHE)}ê°œ\n")
    return {"message": f"ë‰´ìŠ¤ {item.news_id} ìºì‹œì— ì €ì¥ ì™„ë£Œ"}

#ë‰´ìŠ¤ ì¶”ì²œ API
@app.post("/recommend", response_model=RecommendResponse)
def recommend(data: RecommendRequest):
    try:
        user_id = data.user_id
        clicked_ids = set(data.clicked_news_ids)
        
        print(f"ğŸ“Š ì¶”ì²œ ìš”ì²­ ë°›ìŒ - user_id: {user_id}")
        print(f"ğŸ‘† í´ë¦­í•œ ë‰´ìŠ¤ ID: {clicked_ids}")
        print(f"ğŸ“¦ í˜„ì¬ ìºì‹œ ìƒíƒœ - ìºì‹œëœ ë‰´ìŠ¤ ìˆ˜: {len(NEWS_CACHE)}")

        # ì‚¬ìš©ìê°€ ë³¸ ë‰´ìŠ¤ì˜ í‚¤ì›Œë“œ ìˆ˜ì§‘
        user_keywords = set()
        for nid in clicked_ids:
            article = NEWS_CACHE.get(nid)
            if article:
                user_keywords.update(article["keywords"])
                print(f"âœ… ë‰´ìŠ¤ {nid}ì˜ í‚¤ì›Œë“œ: {article['keywords']}")
            else:
                print(f"âš ï¸ ë‰´ìŠ¤ {nid}ê°€ ìºì‹œì— ì—†ìŒ")

        print(f"ğŸ”‘ ìˆ˜ì§‘ëœ ì „ì²´ í‚¤ì›Œë“œ: {user_keywords}")

        # ì¶”ì²œí•  ë‰´ìŠ¤ í›„ë³´
        scored = []
        for nid, article in NEWS_CACHE.items():
            if nid in clicked_ids:
                continue
            matched = list(user_keywords & set(article["keywords"]))
            score = len(matched)
            if score > 0:
                scored.append((score, matched, nid, article))

        # ì •ë ¬ ë° ìƒìœ„ 20ê°œ ì„ ì •
        scored.sort(reverse=True, key=lambda x: x[0])
        top_articles = scored[:20]

        # recommendationsë¥¼ ë¹ˆ ë¦¬ìŠ¤íŠ¸ë¡œ ì´ˆê¸°í™”
        recommendations = []
        
        # ì¶”ì²œ ë‰´ìŠ¤ê°€ ìˆëŠ” ê²½ìš°ì—ë§Œ ì²˜ë¦¬
        if top_articles:
            recommendations = [
                Recommendation(
                    news_id=nid,
                    title=article["title"],
                    url=article["url"],
                    matched_keywords=matched
                )
                for _, matched, nid, article in top_articles
            ]

        print(f"ğŸ“š ì¶”ì²œëœ ë‰´ìŠ¤ ìˆ˜: {len(recommendations)}")
        return RecommendResponse(user_id=user_id, recommendations=recommendations)

    except Exception as e:
        print(f"âŒ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
        import traceback
        print(f"ìƒì„¸ ì˜¤ë¥˜: {traceback.format_exc()}")
        raise

#ìºì‹œ ìƒíƒœ í™•ì¸ API / ì„ íƒ ììœ 
@app.get("/cache/list")
def list_cached_news():
    return {"cached_news_ids": list(NEWS_CACHE.keys())}


#1. FastAPI -> spring ë‰´ìŠ¤ ë“±ë¡ / í¬ë¡¤ë§ëœ ë‰´ìŠ¤ ê¸°ì‚¬ë¥¼ FASTAPIì— ìºì‹œ ì €ì¥
#2. spring -> FastAPI ì‚¬ìš©ì ë§ì¶¤ ì¶”ì²œ ìš”ì²­ / user_id, clicked_news_ids ì „ì†¡
#3. FastAPI input: clicked_news_ids, NEWS_CACHE output: ë™ì¼ í‚¤ì›Œë“œ ë§ì€ ìˆœëŒ€ë¡œ ìƒìœ„ 20ê°œ
#4. ì‚¬ìš©ìê°€ í´ë¦­í•œ ë‰´ìŠ¤ idì— í•´ë‹¹ë˜ëŠ” ê¸°ì‚¬ë¥¼ ì°¾ì•„ í‚¤ì›Œë“œë“¤ë§Œ ëª¨ì•„ì„œ ì‚¬ìš©ì ê´€ì‹¬ í‚¤ì›Œë“œ ì§‘í•© ìƒì„±
#5. ê° ê¸°ì‚¬ë§ˆë‹¤ ì‚¬ìš©ì í‚¤ì›Œë“œì™€ ëª‡ê°œì”© ê²¹ì¹˜ëŠ”ì§€ ê³„ì‚°
#6 ê²¹ì¹˜ëŠ” í‚¤ì›Œë“œê°€ ë†’ì€ ìˆœìœ¼ë¡œ ì •ë ¬, ìƒìœ„ 20ê°œ ì¶”ì²œ
#7. ì‚¬ìš©ì id, ì¶”ì²œ ë‰´ìŠ¤ id, ì¶”ì²œ ë‰´ìŠ¤ ì œëª©, ì¶”ì²œ ë‰´ìŠ¤ url, ì¶”ì²œ ë‰´ìŠ¤ í‚¤ì›Œë“œ ë°˜í™˜
# FastAPI ->spring ì¶”ì²œ ê²½ê³¼ ë°˜í™˜ 

#íŒŒì´ì¬ ì‹¤í–‰ python -m uvicorn crawling_keyword_v3:app --reload --port 8000
#ë¡œì»¬ í˜¸ìŠ¤íŠ¸ http://localhost:8000
#FastAPI http://localhost:8000/docs
